{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhenoCam - Extracting Image Statistics over a Region of Interest (ROI)\n",
    "\n",
    "Here's an iPython notebook to demonstrate the code used in getting stats from an image and ROI mask pair.  Let's start by reading in an image and mask.  The routine processing code is written in python but we've compared similar processing in R, and MATLAB with comparable results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "test_image = 'harvard_2008_08_24_120140.jpg'\n",
    "test_mask = 'harvard_DB_0001_01.tif'\n",
    "\n",
    "# read in mask image and convert to nparray\n",
    "mask_img = Image.open(test_mask)\n",
    "roimask = np.asarray(mask_img, dtype=np.bool8)\n",
    "\n",
    "# read in canopy image\n",
    "img = Image.open(test_image)\n",
    "img.load()\n",
    "\n",
    "# create an image with alpha mask\n",
    "rev_mask_img = ImageOps.invert(mask_img)\n",
    "alpha_data = rev_mask_img.getdata()\n",
    "rev_mask_img.putdata(alpha_data,.4,150)\n",
    "imga = img.copy()\n",
    "fig = plt.figure(figsize=[8,8])\n",
    "imga.putalpha(rev_mask_img)\n",
    "plt.imshow(imga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantity we use for phenological studies is the \"green chromatic coordinate\" or \"gcc\" value.  This is defined as:\n",
    "\n",
    "$$ gcc = \\frac{g_{mean}}{r_{mean} + g_{mean} + b_{mean}} $$\n",
    "\n",
    "To further characterize the distribution of color over the ROi we also calculate the percentiles of the distributions of the digital number (DN) values for each color plane.  The correlations coefficients between the DN's of the different color planes are also calculated.  Here is the function used in standard processing routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roi_stats(im, roimask):\n",
    "    \"\"\"\n",
    "    Function to return a collection of stats for DN values for an image / mask pair.\n",
    "    \"\"\"\n",
    " \n",
    "    # split into bands\n",
    "    (im_r, im_g, im_b) = im.split()\n",
    "\n",
    "    # create numpy arrays with bands\n",
    "    r_array = np.asarray(im_r, dtype=np.int16)\n",
    "    g_array = np.asarray(im_g, dtype=np.int16)\n",
    "    b_array = np.asarray(im_b, dtype=np.int16)\n",
    "\n",
    "    # try applying mask to red image ... if mask and image don't\n",
    "    # have same size this will raise an exception.\n",
    "    try:\n",
    "        r_ma = np.ma.array(r_array,mask=roimask)\n",
    "    except:\n",
    "        errstr = \"Error applying mask to image file.\\n\"\n",
    "        sys.stderr.write(errstr)\n",
    "        return None\n",
    "\n",
    "    # make masked arrays for G,B\n",
    "    g_ma = np.ma.array(g_array,mask=roimask)\n",
    "    b_ma = np.ma.array(b_array,mask=roimask)\n",
    "\n",
    "    # find mean, std\n",
    "    r_vals = r_ma.compressed()\n",
    "    r_mean = r_vals.mean()\n",
    "    r_diff = np.float64(r_vals) - r_mean\n",
    "    r_std = np.sqrt(np.dot(r_diff, r_diff)/r_vals.size)\n",
    "\n",
    "    g_vals = g_ma.compressed()\n",
    "    g_mean = g_vals.mean()\n",
    "    g_diff = np.float64(g_vals) - g_mean\n",
    "    g_std = np.sqrt(np.dot(g_diff, g_diff)/g_vals.size)\n",
    "    \n",
    "    b_vals = b_ma.compressed()\n",
    "    b_mean = b_vals.mean()\n",
    "    b_diff = np.float64(b_vals) - b_mean\n",
    "    b_std = np.sqrt(np.dot(b_diff, b_diff)/b_vals.size)\n",
    "    \n",
    "    # calculate percentiles for each array\n",
    "    r_pcts = np.percentile(r_vals, (5., 10., 25., 50., 75., 90., 95.))\n",
    "    g_pcts = np.percentile(g_vals, (5., 10., 25., 50., 75., 90., 95.))\n",
    "    b_pcts = np.percentile(b_vals, (5., 10., 25., 50., 75., 90., 95.))\n",
    "    \n",
    "    \n",
    "    # calculate covariance \n",
    "    rg_cov = np.dot(r_diff, g_diff)/r_diff.size\n",
    "    gb_cov = np.dot(g_diff, b_diff)/g_diff.size\n",
    "    br_cov = np.dot(b_diff, r_diff)/b_diff.size\n",
    "    \n",
    "    # correlation coefficients\n",
    "    RG_cor = rg_cov/(r_std * g_std)\n",
    "    GB_cor = gb_cov/(g_std * b_std)\n",
    "    BR_cor = br_cov/(b_std * r_std)\n",
    "    \n",
    "    # return list of values\n",
    "    return [{'mean': r_mean, \n",
    "             'stdev': r_std, \n",
    "             'percentiles': r_pcts}, \n",
    "            {'mean': g_mean,\n",
    "             'stdev': g_std, \n",
    "             'percentiles':g_pcts}, \n",
    "             {'mean': b_mean, \n",
    "              'stdev': b_std, \n",
    "              'percentiles': b_pcts}, \n",
    "            RG_cor, GB_cor, BR_cor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call stats function\n",
    "stats = get_roi_stats(img, roimask)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of what these numbers mean let's repeat the above without using a function.  That way we can examine some of the intermediate steps.  In particular it would be nice to examine the distribution of DN values over the ROI for each of the color planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into bands\n",
    "(im_r, im_g, im_b) = img.split()\n",
    "\n",
    "# create numpy arrays with bands\n",
    "r_array = np.asarray(im_r, dtype=np.float64)\n",
    "g_array = np.asarray(im_g, dtype=np.float64)\n",
    "b_array = np.asarray(im_b, dtype=np.float64)\n",
    "brt_array = r_array + g_array + b_array\n",
    "\n",
    "# apply mask to brightness image \n",
    "brt_ma = np.ma.array(brt_array,mask=roimask)\n",
    "\n",
    "# make masked arrays for R,G,B\n",
    "g_ma = np.ma.array(g_array,mask=roimask)\n",
    "r_ma = np.ma.array(r_array,mask=roimask)\n",
    "b_ma = np.ma.array(b_array,mask=roimask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, all I've done so far is split the image into R, G and B components and created a ```numpy``` \"masked array\".  For simple stats, like ```mean```, used in the previous calculations we could just use masked array functions.  Here I'm going to grab the un-masked values and put them in 1-d arrays.  This also makes it easier to do the plotting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's get percentiles for R, G and B\n",
    "nvalues = g_ma.size\n",
    "r_values = r_ma.compressed()\n",
    "g_values = g_ma.compressed()\n",
    "b_values = b_ma.compressed()\n",
    "\n",
    "print(\"Number of pixels in image: {0}\".format(nvalues))\n",
    "print(\"Number of masked pixels: {0}\".format(len(g_values)))\n",
    "\n",
    "# calculate mean, median and std-dev for R, G, B\n",
    "r_mean = np.mean(r_values)\n",
    "r_median = np.median(r_values)\n",
    "r_var = np.var(r_values)\n",
    "r_std = np.std(r_values)\n",
    "g_mean = np.mean(g_values)\n",
    "g_median = np.median(g_values)\n",
    "g_var = np.var(g_values)\n",
    "g_std = np.std(g_values)\n",
    "b_mean = np.mean(b_values)\n",
    "b_median = np.median(b_values)\n",
    "b_var = np.var(b_values)\n",
    "b_std = np.std(b_values)\n",
    "\n",
    "# calculate percentiles for each array\n",
    "r_percentiles = np.percentile(r_values, (5., 10., 25., 50., 75., 90., 95.))\n",
    "g_percentiles = np.percentile(g_values, (5., 10., 25., 50., 75., 90., 95.))\n",
    "b_percentiles = np.percentile(b_values, (5., 10., 25., 50., 75., 90., 95.))\n",
    "\n",
    "# histogram the value for R, G, B\n",
    "fig = plt.figure(figsize=[20,6])\n",
    "\n",
    "r_95 = r_percentiles[-1]\n",
    "r_low = r_mean - r_std\n",
    "r_high = r_mean + r_std\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(r_values,20,color='r')\n",
    "plt.plot([r_mean, r_mean], [0,60000], 'k--', lw=2)\n",
    "plt.plot([r_low, r_low], [0,60000], 'k:', lw=2)\n",
    "plt.plot([r_high, r_high], [0,60000], 'k:', lw=2 )\n",
    "plt.text(150,50000,'mean:      {0:.2f}'.format(r_mean), size=14)\n",
    "plt.text(150,47000,'std. dev.: {0:.2f}'.format(r_std), size=14)\n",
    "plt.xlim([0,255])\n",
    "\n",
    "g_95 = g_percentiles[-1]\n",
    "g_low = g_mean - g_std\n",
    "g_high = g_mean + g_std\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(g_values,20,color='g')\n",
    "plt.plot([g_mean, g_mean], [0,60000], 'k--', lw=2)\n",
    "plt.plot([g_low, g_low], [0,60000], 'k:', lw=2)\n",
    "plt.plot([g_high, g_high], [0,60000], 'k:', lw=2 )\n",
    "plt.text(150,50000,'mean:       {0:.2f}'.format(g_mean), size=14)\n",
    "plt.text(150,47000,'std. dev.:  {0:.2f}'.format(g_std), size=14)\n",
    "plt.xlim([0,255])\n",
    "\n",
    "b_95 = b_percentiles[-1]\n",
    "b_low = b_mean - b_std\n",
    "b_high = b_mean + b_std\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(b_values,20,color='b')\n",
    "plt.plot([b_mean, b_mean], [0,70000], 'k--', lw=2)\n",
    "plt.plot([b_low, b_low], [0,70000], 'k:', lw=2)\n",
    "plt.plot([b_high, b_high], [0,70000], 'k:', lw=2 )\n",
    "plt.text(150,59000,'mean:       {0:.2f}'.format(b_mean), size=14)\n",
    "plt.text(150,55000,'std. dev.:  {0:.2f}'.format(b_std), size=14)\n",
    "\n",
    "plt.xlim([0,255])\n",
    "\n",
    "\n",
    "print(r_mean, r_std, r_median, r_var, r_percentiles)\n",
    "print(g_mean, g_std, g_median, g_var, g_percentiles)\n",
    "print(b_mean, b_std, b_median, b_var, b_percentiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the correlation coefficients is probably best done by showing a scatterplot of DN values for the pixels in the ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the covariance matrix between pairs (R,G), (R,B), (G,B)\n",
    "rg_cov = np.cov(r_values, g_values)\n",
    "rb_cov = np.cov(r_values, b_values)\n",
    "gb_cov = np.cov(g_values, b_values)\n",
    "\n",
    "fig = plt.figure(figsize=[16,4])\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(r_values,g_values,s=2,color='k',alpha=.2,edgecolor='none')\n",
    "plt.xlim([0,255])\n",
    "plt.ylim([0,255])\n",
    "plt.xlabel('R')\n",
    "plt.ylabel('G')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(r_values,b_values,s=2,color='k',alpha=.2,edgecolor='none')\n",
    "plt.xlim([0,255])\n",
    "plt.ylim([0,255])\n",
    "plt.xlabel('R')\n",
    "plt.ylabel('B')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(g_values,b_values,s=2,color='k',alpha=.2,edgecolor='none')\n",
    "plt.xlim([0,255])\n",
    "plt.ylim([0,255])\n",
    "plt.xlabel('G')\n",
    "plt.ylabel('B')\n",
    "\n",
    "RG_cov = rg_cov[0,1]\n",
    "RB_cov = rb_cov[0,1]\n",
    "GB_cov = gb_cov[0,1]\n",
    "RG_cor = RG_cov/(r_std * g_std)\n",
    "RB_cor = RB_cov/(r_std * b_std)\n",
    "GB_cor = GB_cov/(g_std * b_std)\n",
    "\n",
    "print('rg_cov: {0}  rb_cov: {1}  gb_cov: {2}'.format(RG_cov, RB_cov, GB_cov))\n",
    "print('rg_cor: {0}  rb_cor: {1}  gb_cor: {2}'.format(RG_cor, RB_cor, GB_cor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions or Suggestions\n",
    "\n",
    "If you have questions about how the statistics are calculated please feel free to contact me (Tom Milliman) at thomas.milliman@unh.edu.  We would also welcome any suggestions on image processing or comments on our standard products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Here's a [link](http://stackoverflow.com/questions/19391149/numpy-mean-and-variance-from-single-function) to a stackoverflow posting which discusses speeding up calculations of mean and variance.  I used this discussion to help speed up calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
